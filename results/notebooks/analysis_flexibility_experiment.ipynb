{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a307d37",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "Given a flexibility experiment, generates tables and plots for solution quality, learning costs, and adaption costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0774f102",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a3f8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os, csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Path to experimental data. Should contain a 'logs' directory.\n",
    "exp_path = \"../../../flexbench-data/3_NSGA-II_varying_goals/3-2_varying_goals_active_inactive/\"\n",
    "\n",
    "# Source and target paths, in case they are different from the experiment path.\n",
    "source_path = exp_path\n",
    "# target_path = exp_path\n",
    "# source_path = \"../../../flexbench-data/2_baseline_NSGA-II/2-2_tournament/\"\n",
    "target_path = \"../../../flexbench-data/2_baseline_NSGA-II/2-2_tournament/\"\n",
    "\n",
    "# Experiment data.\n",
    "tasks_target = [\"steel\", \"tungsten_alloy\", \"steel_dummy\", \"inconel_718\"]\n",
    "# tasks_source = tasks_target\n",
    "tasks_source = [\n",
    "    \"steel_and_tungsten_alloy\",\n",
    "    \"steel_and_steel_dummy\",\n",
    "    \"steel_and_inconel_718\",\n",
    "    \"tungsten_alloy_and_steel_dummy\",\n",
    "    \"tungsten_alloy_and_inconel_718\",\n",
    "    \"steel_dummy_and_inconel_718\",\n",
    "]\n",
    "repetitions = 100\n",
    "nGens = 50\n",
    "popSize = 100\n",
    "popSizeAdaption = 100\n",
    "\n",
    "# Value for calculating computational effort.\n",
    "z = 0.99\n",
    "\n",
    "# If True, additionally calculates computational effort and evaluations\n",
    "# based on a percentage of the target hypervolume.\n",
    "include_relaxed_target = True\n",
    "percentage = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1c886e",
   "metadata": {},
   "source": [
    "### Load CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5c97ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    \"\"\"Returns an array containing the maximum of each row in 'path'.\"\"\"\n",
    "    print(\"Loading %s...\" % path)\n",
    "    with open(path, \"r\", newline=\"\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        data = np.array([max([float(x) for x in row]) for row in reader])\n",
    "    return data[:repetitions]\n",
    "\n",
    "def load_hypervolume_gens(path, only_increases=True):\n",
    "    \"\"\"\n",
    "    Returns an array that is the mean of columns in 'path', in order to\n",
    "    show the development of the best hypervolume accross generations.\n",
    "    \"\"\"   \n",
    "    print(\"Loading %s...\" % path)\n",
    "    \n",
    "    with open(path, \"r\", newline=\"\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        data = [[float(x) for x in row] for row in reader]\n",
    "        data = data[:repetitions]\n",
    "    \n",
    "    if only_increases: \n",
    "        data_processed = []\n",
    "        for row in data:\n",
    "            row_processed = []\n",
    "            hypervolume_max = 0\n",
    "            for gen in range(nGens+1):\n",
    "                if gen < len(row):\n",
    "                    if row[gen] > hypervolume_max:\n",
    "                        hypervolume_max = row[gen]\n",
    "                row_processed.append(hypervolume_max)\n",
    "            data_processed.append(row_processed)\n",
    "            \n",
    "        return np.mean(np.array(data_processed), axis=0)\n",
    "    \n",
    "    else:\n",
    "        return np.mean(np.array(data), axis=0)\n",
    "\n",
    "def load_evals_relaxed(path_hypervolume_target, path_hypervolume_current, popSize):\n",
    "    \"\"\"\n",
    "    Return an array with the number of evaluations needed for finding a percentage\n",
    "    of target hypervolumes.\n",
    "    \"\"\" \n",
    "    print(\"Loading %s...\" % path_hypervolume_current)\n",
    "    \n",
    "    with open(path_hypervolume_target, \"r\", newline=\"\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        hypervolume_target = np.array([max([float(x) for x in row]) for row in reader]) * relaxed_target\n",
    "        hypervolume_target = hypervolume_target[:repetitions]\n",
    "    \n",
    "    with open(path_hypervolume_current, \"r\", newline=\"\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        data = [[float(x) for x in row] for row in reader]\n",
    "        data = data[:repetitions]\n",
    "    \n",
    "    evals_relaxed = [-1 for _ in range(len(data))]\n",
    "    for idx in range(len(data)):\n",
    "        for gen in range(nGens+1):\n",
    "            if data[idx][gen] >= hypervolume_target[idx]:\n",
    "                evals_relaxed[idx] = (gen+1)*popSize\n",
    "                break\n",
    "    \n",
    "    return np.array(evals_relaxed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5642470d",
   "metadata": {},
   "source": [
    "### Computational Effort (CE) calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50533d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_min_effort(evals, nRuns, nGens, popSize, z) :\n",
    "\n",
    "    #Initialize generations vector.\n",
    "    gensVec = [0 for i in range(nGens+1)]\n",
    "    \n",
    "    #Fill generations vector by iterating evaluations vector.\n",
    "    for x in evals :\n",
    "        if x != -1 :\n",
    "            idx = int(x/popSize) - 1\n",
    "            gensVec[idx] = gensVec[idx] + 1\n",
    "    \n",
    "    #Calculate cumulative vector.\n",
    "    for i in range(1,len(gensVec)) :\n",
    "        gensVec[i] = gensVec[i] + gensVec[i-1]\n",
    "    \n",
    "    #Minimum effort initial infinite.\n",
    "    minEff = float('+Inf')\n",
    "    \n",
    "    #For each generation.\n",
    "    for i in range(len(gensVec)) :\n",
    "    \n",
    "        #Calculate probability based on frequencies.\n",
    "        prob = gensVec[i]/float(nRuns)\n",
    "        \n",
    "        if prob == 0:\n",
    "            prob = 1e-7\n",
    "        \n",
    "        #Calculate effort.\n",
    "        if prob == 1.0 :\n",
    "            r = 1.0\n",
    "        else :\n",
    "            r = np.ceil(np.log(1-z)/np.log(1-prob))\n",
    "            \n",
    "        currEff = popSize * (i+1) * r\n",
    "        \n",
    "        #Update minimum effort.\n",
    "        if currEff < minEff :\n",
    "            minEff = currEff\n",
    "        \n",
    "    #Return minimum effort.\n",
    "    return minEff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aadd14",
   "metadata": {},
   "source": [
    "### Generate tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75252200",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_empty_table():\n",
    "    return [[\"\",\"From Scratch\"]+tasks_source]\n",
    "\n",
    "def aggregated_table(table_original):\n",
    "    \"\"\"Build table with aggregated flexibility measures.\"\"\"\n",
    "    # Initialize table.\n",
    "    table_aggregated = [[\"\", \"Worst Case\", \"Average Case\", \"Best Case\"]]\n",
    "    \n",
    "    # Values from scratch.\n",
    "    values = [table_original[i+1][1] for i in range(len(tasks_target))]\n",
    "    table_aggregated.append([\"From Scratch\", max(values), np.mean(values), min(values)])\n",
    "    \n",
    "    # Adaption for each material individually.\n",
    "    for idx in range(len(tasks_target)):\n",
    "        values = [\n",
    "            table_original[idx+1][i+2] \n",
    "            for i in range(len(tasks_source))\n",
    "            if table_original[idx+1][i+2] != \"-\"\n",
    "        ]\n",
    "        table_aggregated.append([\n",
    "            \"Adaption to %s\"%tasks_target[idx],\n",
    "            max(values),\n",
    "            np.mean(values),\n",
    "            min(values)\n",
    "        ])\n",
    "    \n",
    "    # Adaption for all.\n",
    "    worst_case = max([table_aggregated[i+2][1] for i in range(len(tasks_target))])\n",
    "    avg_case = np.mean([table_aggregated[i+2][2] for i in range(len(tasks_target))])\n",
    "    best_case = min([table_aggregated[i+2][3] for i in range(len(tasks_target))])\n",
    "    table_aggregated.append([\"Adaption\", worst_case, avg_case, best_case])\n",
    "    \n",
    "    return table_aggregated\n",
    "\n",
    "def save_table(table, path):\n",
    "    print(\"Saving %s...\" % path)\n",
    "    with open(path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f, delimiter=\",\")\n",
    "        writer.writerows(table)\n",
    "\n",
    "def gen_all_tables():\n",
    "    # Initialize tables.\n",
    "    table_comp_effort = gen_empty_table()\n",
    "    table_evals_avg = gen_empty_table()\n",
    "    table_evals_std = gen_empty_table()\n",
    "    table_hypervolumes_avg = gen_empty_table()\n",
    "    table_hypervolumes_std = gen_empty_table()\n",
    "    table_success = gen_empty_table()\n",
    "    table_success_init = gen_empty_table()\n",
    "\n",
    "    # For each material.\n",
    "    for target in tasks_target:\n",
    "\n",
    "        # Initialize rows.\n",
    "        row_comp_effort = [target]\n",
    "        row_evals_avg = [target]\n",
    "        row_evals_std = [target]\n",
    "        row_hypervolumes_avg = [target]\n",
    "        row_hypervolumes_std = [target]\n",
    "        row_success = [target]\n",
    "        row_success_init = [target]\n",
    "\n",
    "        # Load data from scratch.\n",
    "        target_hypervolumes = load_data(load_path_target+target+\"/training/hypervolume.csv\")\n",
    "        if relaxed_target >= 1.0:\n",
    "            target_costs = load_data(load_path_target+target+\"/training/cost.csv\")\n",
    "        else:\n",
    "            target_costs = load_evals_relaxed(\n",
    "                load_path_target+target+\"/training/hypervolume.csv\",\n",
    "                load_path_target+target+\"/training/hypervolume.csv\",\n",
    "                popSize,\n",
    "            )\n",
    "\n",
    "        # Add computational effort.\n",
    "        row_comp_effort.append(calc_min_effort(target_costs,repetitions,nGens,popSize,z))\n",
    "\n",
    "        # Add number of evaluations from scratch (avg and std).\n",
    "        row_evals_avg.append(np.mean(np.array([x for x in target_costs if x != -1])))\n",
    "        row_evals_std.append(np.std(np.array([x for x in target_costs if x != -1])))\n",
    "\n",
    "        # Add hypervolume from scratch (avg and std).\n",
    "        row_hypervolumes_avg.append(np.mean(target_hypervolumes))\n",
    "        row_hypervolumes_std.append(np.std(target_hypervolumes))\n",
    "\n",
    "        # Success and succes on initialization empty for source.\n",
    "        row_success.append(\"-\")\n",
    "        row_success_init.append(\"-\")\n",
    "\n",
    "        # For each combination.\n",
    "        for source in tasks_source:\n",
    "\n",
    "            # Empty if target is source.\n",
    "            if target in source.split(\"_and_\"):\n",
    "                row_comp_effort.append(\"-\")\n",
    "                row_evals_avg.append(\"-\")\n",
    "                row_evals_std.append(\"-\")\n",
    "                row_hypervolumes_avg.append(\"-\")\n",
    "                row_hypervolumes_std.append(\"-\")\n",
    "                row_success.append(\"-\")\n",
    "                row_success_init.append(\"-\")\n",
    "\n",
    "            else:\n",
    "\n",
    "                # Load adaption data.\n",
    "                adapt_hypervolumes = load_data(load_path+source+\"_to_\"+target+\"/adaption/hypervolume.csv\")\n",
    "                if relaxed_target >= 1.0:\n",
    "                    adapt_costs = load_data(load_path+source+\"_to_\"+target+\"/adaption/cost.csv\")\n",
    "                else:\n",
    "                    adapt_costs = load_evals_relaxed(\n",
    "                        load_path_target+target+\"/training/hypervolume.csv\",\n",
    "                        load_path+source+\"_to_\"+target+\"/adaption/hypervolume.csv\",\n",
    "                        popSizeAdaption,\n",
    "                    )\n",
    "\n",
    "                # Add computational effort.\n",
    "                row_comp_effort.append(calc_min_effort(adapt_costs,repetitions,nGens,popSizeAdaption,z))\n",
    "\n",
    "                # Add number of evaluations for adaption (avg and std).\n",
    "                row_evals_avg.append(np.mean(np.array([x for x in adapt_costs if x != -1])))\n",
    "                row_evals_std.append(np.std(np.array([x for x in adapt_costs if x != -1])))\n",
    "\n",
    "                # Add hypervolume for adaption (avg and std).\n",
    "                row_hypervolumes_avg.append(np.mean(adapt_hypervolumes))\n",
    "                row_hypervolumes_std.append(np.std(adapt_hypervolumes))\n",
    "\n",
    "                # Add success.\n",
    "                row_success.append(len([x for x in adapt_costs if x != -1])/len(adapt_costs))\n",
    "\n",
    "                # Add percentage successfull already in initialization.\n",
    "                row_success_init.append(len([x for x in adapt_costs if x == 100])/len(adapt_costs))\n",
    "\n",
    "        # Add rows to tables.\n",
    "        table_comp_effort.append(row_comp_effort)\n",
    "        table_evals_avg.append(row_evals_avg)\n",
    "        table_evals_std.append(row_evals_std)\n",
    "        table_hypervolumes_avg.append(row_hypervolumes_avg)\n",
    "        table_hypervolumes_std.append(row_hypervolumes_std)\n",
    "        table_success.append(row_success)\n",
    "        table_success_init.append(row_success_init)\n",
    "\n",
    "    table_comp_effort_aggr = aggregated_table(table_comp_effort)\n",
    "    table_evals_avg_aggr = aggregated_table(table_evals_avg)\n",
    "\n",
    "    save_table(table_comp_effort, save_path+\"table_min_comp_effort.csv\")\n",
    "    save_table(table_evals_avg, save_path+\"table_evals_avg.csv\")\n",
    "    save_table(table_comp_effort_aggr, save_path+\"table_min_comp_effort_aggregated.csv\")\n",
    "    save_table(table_evals_avg_aggr, save_path+\"table_evals_avg_aggregated.csv\")\n",
    "    save_table(table_evals_std, save_path+\"table_evals_std.csv\")\n",
    "    save_table(table_hypervolumes_avg, save_path+\"table_hypervolumes_avg.csv\")\n",
    "    save_table(table_hypervolumes_std, save_path+\"table_hypervolumes_std.csv\")\n",
    "    save_table(table_success, save_path+\"table_success.csv\")\n",
    "    save_table(table_success_init, save_path+\"table_success_init.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35fb999",
   "metadata": {},
   "source": [
    "### Generate plots (hypervolume accross generations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d35ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_plots_hypervolume_all():\n",
    "    # Initialize subplots.\n",
    "    px = 1/plt.rcParams['figure.dpi']\n",
    "    fig, axsVec = plt.subplots(nrows=2, ncols=2, figsize=(1000*px,600*px), constrained_layout=True)\n",
    "    # fig, axsVec = plt.subplots(nrows=2, ncols=2, figsize=(15,10), constrained_layout=True)\n",
    "    axsVec = [axs for axs in np.array(axsVec).flat]\n",
    "\n",
    "    # For each target material.\n",
    "    for idx in range(len(tasks_target)):\n",
    "\n",
    "        # Select subplot.\n",
    "        axs = axsVec[idx]\n",
    "\n",
    "        # Load data for search from scratch.\n",
    "        data = load_hypervolume_gens(load_path_target+tasks_target[idx]+\"/training/hypervolume.csv\")\n",
    "\n",
    "        # Add to subplot.\n",
    "        axs.scatter(x=[gen for gen in range(len(data))], y=data, label=\"From scratch\", marker='o')\n",
    "\n",
    "        # Markers to be used.\n",
    "        markers = ['s', '*', 'X', 'D']\n",
    "        idxMarker = 0\n",
    "        \n",
    "        # For each source material.\n",
    "        for material_source in tasks_source:\n",
    "\n",
    "            # If it's not the source material.\n",
    "            if tasks_target[idx] not in material_source.split(\"_and_\"):\n",
    "\n",
    "                # Load data for adaption.\n",
    "                data = load_hypervolume_gens(\n",
    "                    load_path+material_source+\"_to_\"+tasks_target[idx]+\"/adaption/hypervolume.csv\"\n",
    "                )\n",
    "\n",
    "                # Add to subplot.\n",
    "                axs.scatter(\n",
    "                    x=[gen for gen in range(len(data))], \n",
    "                    y=data, \n",
    "                    label=\"Source=%s\"%material_source, \n",
    "                    marker=markers[idxMarker],\n",
    "                )\n",
    "                \n",
    "                idxMarker = idxMarker + 1\n",
    "\n",
    "        # Add subplot title, labels, and legends.\n",
    "        fontsize = 12\n",
    "        fontsize_legend = 10\n",
    "        axs.set_xlabel(\"Generations\", fontsize=fontsize, fontweight='bold')\n",
    "        axs.set_ylabel(\"Hypervolume\", fontsize=fontsize, fontweight='bold')\n",
    "        axs.set_title(\"Target=%s\" % tasks_target[idx], fontsize=fontsize, fontweight='bold')\n",
    "        axs.tick_params(axis='x', labelsize=fontsize)\n",
    "        axs.tick_params(axis='y', labelsize=fontsize)\n",
    "        axs.legend(fontsize=fontsize_legend)\n",
    "\n",
    "    # Save plot.\n",
    "    fig.savefig(save_path+\"plots_hypervolume.eps\")\n",
    "    print(\"Plots saved in %s.\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2020ffe",
   "metadata": {},
   "source": [
    "### Generate plots, only for source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d026a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_plots_hypervolume_source():\n",
    "\n",
    "    # Initialize subplots.\n",
    "    fig, axsVec = plt.subplots(nrows=2, ncols=3, figsize=(20,10), constrained_layout=True)\n",
    "    axsVec = [axs for axs in np.array(axsVec).flat]\n",
    "\n",
    "    # For each source material.\n",
    "    for idx in range(len(tasks_source)):\n",
    "\n",
    "        # Select subplot.\n",
    "        axs = axsVec[idx]\n",
    "\n",
    "        # Load data for search from scratch.\n",
    "        data = load_hypervolume_gens(\n",
    "            load_path_source+tasks_source[idx]+\"/training/hypervolume.csv\",\n",
    "            only_increases=False,\n",
    "        )\n",
    "\n",
    "        # Add to subplot.\n",
    "        axs.scatter(x=[gen for gen in range(len(data))], y=data)\n",
    "\n",
    "        # Add subplot title, labels, and legends.\n",
    "        fontsize = 20\n",
    "        axs.set_xlabel(\"Generations\", fontsize=fontsize, fontweight='bold')\n",
    "        axs.set_ylabel(\"Hypervolume\", fontsize=fontsize, fontweight='bold')\n",
    "        axs.set_title(tasks_source[idx], fontsize=fontsize, fontweight='bold')\n",
    "        axs.tick_params(axis='x', labelsize=fontsize)\n",
    "        axs.tick_params(axis='y', labelsize=fontsize)\n",
    "\n",
    "    # Save plot.\n",
    "    fig.savefig(save_path+\"plots_hypervolume_source.eps\")\n",
    "    print(\"Plots saved in %s.\" % save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49afbda",
   "metadata": {},
   "source": [
    "### Runs analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953b5ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = exp_path + \"logs/\"\n",
    "load_path_source = source_path + \"logs/\"\n",
    "load_path_target = target_path + \"logs/\"\n",
    "save_path = exp_path + \"analysis/\"\n",
    "\n",
    "try:\n",
    "    os.mkdir(save_path)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "relaxed_target = 1.0\n",
    "\n",
    "gen_all_tables()\n",
    "gen_plots_hypervolume_all()\n",
    "gen_plots_hypervolume_source()\n",
    "\n",
    "if include_relaxed_target:\n",
    "    save_path = exp_path + \"analysis_relaxed_target_%.2f/\" % percentage\n",
    "\n",
    "    try:\n",
    "        os.mkdir(save_path)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n",
    "    relaxed_target = percentage\n",
    "\n",
    "    gen_all_tables()\n",
    "    gen_plots_hypervolume_all()\n",
    "    gen_plots_hypervolume_source()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
