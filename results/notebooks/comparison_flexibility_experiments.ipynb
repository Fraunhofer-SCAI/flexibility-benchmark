{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b54f4d5",
   "metadata": {},
   "source": [
    "### Description\n",
    "\n",
    "Given two or more flexibility experiments, generates tables comparing the aggregate flexibility measures (worst, average, and best learning and adaption costs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be4a6a5",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba29a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import stats\n",
    "import scikit_posthocs as sp\n",
    "\n",
    "# Path where to store anlysis results.\n",
    "save_path = '../../../flexbench-data/3_NSGA-II_varying_goals/comp_baseline_varying_goals_active_inactive/'\n",
    "\n",
    "# Path to experimental data (each directory should contain a 'logs' subdirectory).\n",
    "# Baseline refers to where the data for learning from scratch is.\n",
    "# Adaption refers to where the data for adaption is.\n",
    "baseline_paths = {\n",
    "    'Baseline (from scratch)': {\n",
    "        'path' : '../../../flexbench-data/2_baseline_NSGA-II/2-2_tournament/',\n",
    "        'popSize': 100,\n",
    "        'nGens': 50,\n",
    "    },\n",
    "}\n",
    "\n",
    "adaption_paths = {\n",
    "    'Baseline (adaption)' : {\n",
    "        'path' : '../../../flexbench-data/2_baseline_NSGA-II/2-2_tournament/',\n",
    "        'path_target': '../../../flexbench-data/2_baseline_NSGA-II/2-2_tournament/',\n",
    "        'popSize': 100,\n",
    "        'nGens': 50,\n",
    "    },\n",
    "    'Varying Goals (adaption)' : {\n",
    "        'path' : '../../../flexbench-data/3_NSGA-II_varying_goals/3-1_varying_goals/',\n",
    "        'path_target': '../../../flexbench-data/2_baseline_NSGA-II/2-2_tournament/',\n",
    "        'popSize': 100,\n",
    "        'nGens': 50,\n",
    "    },\n",
    "    'Active-Inactive (adaption)' : {\n",
    "        'path' : '../../../flexbench-data/3_NSGA-II_varying_goals/3-2_varying_goals_active_inactive/',\n",
    "        'path_target': '../../../flexbench-data/2_baseline_NSGA-II/2-2_tournament/',\n",
    "        'popSize': 100,\n",
    "        'nGens': 50,\n",
    "    },\n",
    "}\n",
    "\n",
    "# Target tasks. Adaption are identified later for each directory.\n",
    "tasks_target = [\"steel\", \"tungsten_alloy\", \"steel_dummy\", \"inconel_718\"]\n",
    "\n",
    "# Number of repetitions of experiments.\n",
    "repetitions = 100\n",
    "\n",
    "# Value for calculating computational effort (percentage of certainty).\n",
    "z = 0.99\n",
    "\n",
    "# If True, additionally calculates computational effort and evaluations\n",
    "# based on a percentage of the target hypervolume.\n",
    "relaxed_include = True\n",
    "relaxed_percentage = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26c1479",
   "metadata": {},
   "source": [
    "### Functions for loading data and processing cost data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220ff188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_evals_relaxed(\n",
    "    path_hypervolume_target,\n",
    "    path_hypervolume_current,\n",
    "    repetitions,\n",
    "    nGens,\n",
    "    popSize,\n",
    "    relaxed_percentage,\n",
    "):\n",
    "    \"\"\"\n",
    "    Return an array with the number of evaluations needed for finding a percentage\n",
    "    of target hypervolumes.\n",
    "    \"\"\" \n",
    "    print(\"Loading %s...\" % path_hypervolume_current)\n",
    "    \n",
    "    with open(path_hypervolume_target, \"r\", newline=\"\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        hypervolume_target = np.array([max([float(x) for x in row]) for row in reader]) * relaxed_percentage\n",
    "        hypervolume_target = hypervolume_target[:repetitions]\n",
    "    \n",
    "    with open(path_hypervolume_current, \"r\", newline=\"\") as f:\n",
    "        reader = csv.reader(f, delimiter=\",\")\n",
    "        data = [[float(x) for x in row] for row in reader]\n",
    "        data = data[:repetitions]\n",
    "    \n",
    "    evals_relaxed = [-1 for _ in range(len(data))]\n",
    "    for idx in range(len(data)):\n",
    "        for gen in range(nGens+1):\n",
    "            if data[idx][gen] >= hypervolume_target[idx]:\n",
    "                evals_relaxed[idx] = (gen+1)*popSize\n",
    "                break\n",
    "    \n",
    "    return np.array(evals_relaxed)\n",
    "    \n",
    "def calc_min_effort(evals, nRuns, nGens, popSize, z):\n",
    "    '''Calculates the minimum number of evaluations for suceeding with probability z.'''\n",
    "\n",
    "    #Initialize generations vector.\n",
    "    gensVec = [0 for i in range(nGens+1)]\n",
    "    \n",
    "    #Fill generations vector by iterating evaluations vector.\n",
    "    for x in evals :\n",
    "        if x != -1 :\n",
    "            idx = int(x/popSize) - 1\n",
    "            gensVec[idx] = gensVec[idx] + 1\n",
    "    \n",
    "    #Calculate cumulative vector.\n",
    "    for i in range(1,len(gensVec)) :\n",
    "        gensVec[i] = gensVec[i] + gensVec[i-1]\n",
    "    \n",
    "    #Minimum effort initially infinite.\n",
    "    minEff = float('+Inf')\n",
    "    \n",
    "    #For each generation.\n",
    "    for i in range(len(gensVec)) :\n",
    "    \n",
    "        #Calculate probability based on frequencies.\n",
    "        prob = gensVec[i]/float(nRuns)\n",
    "        \n",
    "        if prob == 0:\n",
    "            prob = 1e-7\n",
    "        \n",
    "        #Calculate effort.\n",
    "        if prob == 1.0 :\n",
    "            r = 1.0\n",
    "        else :\n",
    "            r = np.ceil(np.log(1-z)/np.log(1-prob))\n",
    "            \n",
    "        currEff = popSize * (i+1) * r\n",
    "        \n",
    "        #Update minimum effort.\n",
    "        if currEff < minEff :\n",
    "            minEff = currEff\n",
    "        \n",
    "    #Return minimum effort.\n",
    "    return minEff\n",
    "\n",
    "def gen_data_dict(baseline_paths, adaption_paths, tasks_target, repetitions, z, relaxed_percentage):\n",
    "    '''Returns a dictionary with baseline and adaption experiments as keys and CE as values.'''\n",
    "\n",
    "    # Initialize data dict.\n",
    "    data_dict = {}\n",
    "    \n",
    "    # For each baseline experiment.\n",
    "    for exp in baseline_paths.keys():\n",
    "    \n",
    "        # Initilialize entry.\n",
    "        data_dict[exp] = {}\n",
    "        \n",
    "        # For each target.\n",
    "        for target in tasks_target:\n",
    "        \n",
    "            # Read evals, calculate and store CE.\n",
    "            evals = load_evals_relaxed(\n",
    "                baseline_paths[exp]['path'] + 'logs/' + target + '/training/hypervolume.csv',\n",
    "                baseline_paths[exp]['path'] + 'logs/' + target + '/training/hypervolume.csv',\n",
    "                repetitions,\n",
    "                baseline_paths[exp]['nGens'],\n",
    "                baseline_paths[exp]['popSize'],\n",
    "                relaxed_percentage,\n",
    "            )\n",
    "            CE = calc_min_effort(\n",
    "                evals,\n",
    "                repetitions,\n",
    "                baseline_paths[exp]['nGens'],\n",
    "                baseline_paths[exp]['popSize'],\n",
    "                z,\n",
    "            )\n",
    "            data_dict[exp][target] = CE\n",
    "    \n",
    "    # For each adaption experiment.\n",
    "    for exp in adaption_paths.keys():\n",
    "    \n",
    "        # Initialize entry.\n",
    "        data_dict[exp] = {}\n",
    "        \n",
    "        # For each target.\n",
    "        for target in tasks_target:\n",
    "        \n",
    "            # Identify pairs of source to target.\n",
    "            pairs = [\n",
    "                pair\n",
    "                for pair in os.listdir(adaption_paths[exp]['path'] + 'logs/')\n",
    "                if ('_to_' in pair) and (pair.split('_to_')[-1] == target)\n",
    "            ]\n",
    "            \n",
    "            # For each pair.\n",
    "            for pair in pairs:\n",
    "            \n",
    "                # Read evals, calculate and store CE.\n",
    "                evals = load_evals_relaxed(\n",
    "                    adaption_paths[exp]['path_target'] + 'logs/' + target + '/training/hypervolume.csv',\n",
    "                    adaption_paths[exp]['path'] + 'logs/' + pair + '/adaption/hypervolume.csv',\n",
    "                    repetitions,\n",
    "                    adaption_paths[exp]['nGens'],\n",
    "                    adaption_paths[exp]['popSize'],\n",
    "                    relaxed_percentage,\n",
    "                )\n",
    "                CE = calc_min_effort(\n",
    "                    evals,\n",
    "                    repetitions,\n",
    "                    adaption_paths[exp]['nGens'],\n",
    "                    adaption_paths[exp]['popSize'],\n",
    "                    z,\n",
    "                )\n",
    "                data_dict[exp][pair] = CE\n",
    "                \n",
    "    # Return data dict.\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c79a8d2",
   "metadata": {},
   "source": [
    "### Functions for generating comparative table of aggregated measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0203c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_table(table, path):\n",
    "    print(\"Saving %s...\" % path)\n",
    "    with open(path, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f, delimiter=\",\")\n",
    "        writer.writerows(table)\n",
    "\n",
    "def gen_aggregated_table(data_dict, save_path):\n",
    "    '''Given a data dict, generates a table with worst, average, and best cases.'''\n",
    "    \n",
    "    # Initialize table.\n",
    "    table = [['Algorithm', 'Worst Case', 'Average Case', 'Best Case']]\n",
    "    \n",
    "    # For each experiment, calculate measures and add row to table.\n",
    "    for exp in data_dict.keys():\n",
    "        values = [data_dict[exp][task] for task in data_dict[exp].keys()]\n",
    "        worst_case = max(values)\n",
    "        avg_case = np.mean(values)\n",
    "        best_case = min(values)\n",
    "        table.append([exp, worst_case, avg_case, best_case])\n",
    "    \n",
    "    # Save table.\n",
    "    save_table(table, save_path + 'table_aggregated_values.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9361533",
   "metadata": {},
   "source": [
    "### Functions for generating table with Friedmann and post-hoc Nemeyi statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71728a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_statistical_tests(data_dict, save_path):\n",
    "    \n",
    "    # Format data.\n",
    "    table_values = [\n",
    "        [data_dict[exp][task] for task in data_dict[exp].keys()]\n",
    "        for exp in data_dict.keys()\n",
    "    ]\n",
    "    \n",
    "    # Fix shorter rows.\n",
    "    # Assumes adaption rows have more values because of different source x target pairs.\n",
    "    # Assumes adaption rows have all the same length, with equal number of values per target.\n",
    "    len_baseline = min([len(row) for row in table_values])\n",
    "    len_adaption = max([len(row) for row in table_values])\n",
    "    n_per_target = int(len_adaption/len_baseline)\n",
    "    for idx in range(len(table_values)):\n",
    "        if len(table_values[idx]) < len_adaption:\n",
    "            table_values[idx] = np.repeat(table_values[idx], n_per_target)\n",
    "    \n",
    "    # Run Friedman test.\n",
    "    result_friedman = stats.friedmanchisquare(*table_values)\n",
    "    \n",
    "    # Generate mean rankings.\n",
    "    result_ranks = np.mean(stats.rankdata(table_values, axis=0), axis=1)\n",
    "    \n",
    "    # Run Nemenyi test.\n",
    "    result_nemenyi = sp.posthoc_nemenyi_friedman(np.array(table_values).T)\n",
    "    \n",
    "    # Build tables.\n",
    "    table_friedman = [\n",
    "        ['Friedman Statistic', result_friedman.statistic],\n",
    "        ['Friedman p-value', result_friedman.pvalue],\n",
    "    ]\n",
    "    \n",
    "    table_mean_rankings = [\n",
    "        [exp for exp in data_dict.keys()],\n",
    "        [rank for rank in result_ranks],\n",
    "    ]\n",
    "    \n",
    "    table_nemenyi = pd.DataFrame(\n",
    "        result_nemenyi.values, \n",
    "        index = [exp for exp in data_dict.keys()],\n",
    "        columns = [exp for exp in data_dict.keys()],\n",
    "    )\n",
    "    \n",
    "    # Save tables.\n",
    "    save_table(table_friedman, save_path + 'table_friedman.csv')\n",
    "    save_table(table_mean_rankings, save_path + 'table_mean_rankings.csv')\n",
    "    \n",
    "    print(\"Saving %s...\" % save_path + 'table_nemenyi.csv')\n",
    "    table_nemenyi.to_csv(save_path + 'table_nemenyi.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d055e2f",
   "metadata": {},
   "source": [
    "### Runs complete analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e84fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create main analysis directory.\n",
    "try:\n",
    "    os.mkdir(save_path)\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "# Subdirectory without relaxed target.\n",
    "try:\n",
    "    os.mkdir(save_path + 'analysis/')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "# Build data dict.\n",
    "data_dict = gen_data_dict(baseline_paths, adaption_paths, tasks_target, repetitions, z, 1.0)\n",
    "\n",
    "# Generate and save tables with aggretated measures.\n",
    "gen_aggregated_table(data_dict, save_path + 'analysis/')\n",
    "\n",
    "# Generate and save tables with statistical tests.\n",
    "perform_statistical_tests(data_dict, save_path + 'analysis/')\n",
    "\n",
    "# If including relaxed target.\n",
    "if relaxed_include:\n",
    "\n",
    "    # Subdirectory with relaxed target.\n",
    "    try:\n",
    "        os.mkdir(save_path + 'analysis_relaxed_target_%.2f/' % relaxed_percentage)\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    # Build data dict.\n",
    "    data_dict = gen_data_dict(baseline_paths, adaption_paths, tasks_target, repetitions, z, relaxed_percentage)\n",
    "\n",
    "    # Generate and save tables with aggretated measures.\n",
    "    gen_aggregated_table(data_dict, save_path + 'analysis_relaxed_target_%.2f/' % relaxed_percentage)\n",
    "\n",
    "    # Generate and save tables with statistical tests.\n",
    "    perform_statistical_tests(data_dict, save_path + 'analysis_relaxed_target_%.2f/' % relaxed_percentage)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
